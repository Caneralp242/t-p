{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bh9vsC4KrCzg","outputId":"1202b268-31b0-4209-9fe3-4515c168610f"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["import required packages\n","NEXT:Initialize image data generator with rescaling\n","DONE:Initialize image data generator with rescaling\n","NEXT:Preprocess all train images\n","Found 28836 images belonging to 7 classes.\n","DONE:Preprocess all test images\n","NEXT:Preprocess all test images\n","Found 7178 images belonging to 7 classes.\n","DONE:Preprocess all train images\n","NEXT:create model structure\n","DONE:create model structure\n","NEXT:Train the neural network/model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","375/448 [========================>.....] - ETA: 33:39 - loss: 1.7613 - accuracy: 0.2754"]}],"source":["\n","print(\"import required packages\")\n","import cv2\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n","#from keras.optimizers import Adam\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","print(\"NEXT:Initialize image data generator with rescaling\")\n","train_data_gen = ImageDataGenerator(rescale=1./255)\n","validation_data_gen = ImageDataGenerator(rescale=1./255)\n","print(\"DONE:Initialize image data generator with rescaling\")\n","\n","print(\"NEXT:Preprocess all train images\")\n","train_generator = train_data_gen.flow_from_directory(\n","        '/content/drive/My Drive/EMOTİONS/train',\n","        target_size=(48, 48),\n","        batch_size=64,\n","        color_mode=\"grayscale\",\n","        class_mode='categorical')\n","print(\"DONE:Preprocess all test images\")\n","\n","\n","print(\"NEXT:Preprocess all test images\")\n","validation_generator = validation_data_gen.flow_from_directory(\n","        '/content/drive/My Drive/EMOTİONS/test',\n","        target_size=(48, 48),\n","        batch_size=64,\n","        color_mode=\"grayscale\",\n","        class_mode='categorical')\n","print(\"DONE:Preprocess all train images\")\n","\n","\n","print(\"NEXT:create model structure\")\n","emotion_model = Sequential()\n","\n","emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n","emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n","emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n","emotion_model.add(Dropout(0.25))\n","\n","emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n","emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n","emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n","emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n","emotion_model.add(Dropout(0.25))\n","print(\"DONE:create model structure\")\n","\n","\n","emotion_model.add(Flatten())\n","emotion_model.add(Dense(1024, activation='relu'))\n","emotion_model.add(Dropout(0.5))\n","emotion_model.add(Dense(7, activation='softmax'))\n","\n","cv2.ocl.setUseOpenCL(False)\n","\n","emotion_model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n","\n","print(\"NEXT:Train the neural network/model\")\n","emotion_model_info = emotion_model.fit_generator(\n","        train_generator,\n","        steps_per_epoch=28709 // 64,\n","        epochs=5,\n","        validation_data=validation_generator,\n","        validation_steps=7178 // 64)\n","print(\"DONE:Train the neural network/model\")\n","\n","\n","print(\"NEXT: save model structure in jason file\")\n","model_json = emotion_model.to_json()\n","with open(\"emotion_model.json\", \"w\") as json_file:\n","    json_file.write(model_json)\n","print(\"DONE: save model structure in jason file\")\n","\n","\n","print(\"NEXT: save trained model weight in .h5 file\")\n","emotion_model.save_weights('emotion_model.h5')\n","print(\"DONE: save trained model weight in .h5 file\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yROxugkltPuv"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from keras.models import model_from_json\n","\n","\n","emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n","\n","# load json and create model\n","json_file = open('/content/emotion_model.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","emotion_model = model_from_json(loaded_model_json)\n","\n","# load weights into new model\n","emotion_model.load_weights(\"/content/emotion_model.h5\")\n","print(\"Loaded model from disk\")\n","\n","# start the webcam feed\n","cap = cv2.VideoCapture(\"/content/drive/MyDrive/EMOTİONS/emotions.HEIC\")\n","cv2.imread(cap)\n","\n","\n","\n","while True:\n","    # Find haar cascade to draw bounding box around face\n","    ret, frame = cap.read()\n","    \n","    if not ret:\n","        break\n","    face_detector = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n","    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","    # detect faces available on camera\n","    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n","\n","    # take each face available on the camera and Preprocess it\n","    for (x, y, w, h) in num_faces:\n","        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n","        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n","        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n","\n","        # predict the emotions\n","        emotion_prediction = emotion_model.predict(cropped_img)\n","        maxindex = int(np.argmax(emotion_prediction))\n","        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n","\n","    cv2.imshow('Emotion Detection', frame)\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"EMOTİONS.ipynb","provenance":[],"mount_file_id":"1dzNUbB6xe-GkFRCb7FxoI3sklY7TsAw2","authorship_tag":"ABX9TyN3I7MgzY52Kvi20B5uzHYi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}